{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\my_own_app\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\my_own_app\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\my_own_app\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37500 samples, validate on 12500 samples\n",
      "Epoch 1/20\n",
      "37500/37500 [==============================] - 267s 7ms/step - loss: 1.5269 - acc: 0.3454 - val_loss: 1.1950 - val_acc: 0.4618\n",
      "Epoch 2/20\n",
      "37500/37500 [==============================] - 260s 7ms/step - loss: 1.1255 - acc: 0.4983 - val_loss: 0.9616 - val_acc: 0.5703\n",
      "Epoch 3/20\n",
      "37500/37500 [==============================] - 258s 7ms/step - loss: 0.9172 - acc: 0.5785 - val_loss: 0.8818 - val_acc: 0.5960\n",
      "Epoch 4/20\n",
      "37500/37500 [==============================] - 286s 8ms/step - loss: 0.7896 - acc: 0.6275 - val_loss: 0.7164 - val_acc: 0.6470\n",
      "Epoch 5/20\n",
      "37500/37500 [==============================] - 261s 7ms/step - loss: 0.7047 - acc: 0.6556 - val_loss: 0.7408 - val_acc: 0.6514\n",
      "Epoch 6/20\n",
      "37500/37500 [==============================] - 250s 7ms/step - loss: 0.6405 - acc: 0.6822 - val_loss: 0.6484 - val_acc: 0.6778\n",
      "Epoch 7/20\n",
      "37500/37500 [==============================] - 294s 8ms/step - loss: 0.5869 - acc: 0.6984 - val_loss: 0.6287 - val_acc: 0.6878\n",
      "Epoch 8/20\n",
      "37500/37500 [==============================] - 276s 7ms/step - loss: 0.5486 - acc: 0.7125 - val_loss: 0.6022 - val_acc: 0.6956\n",
      "Epoch 9/20\n",
      "37500/37500 [==============================] - 230s 6ms/step - loss: 0.5097 - acc: 0.7237 - val_loss: 0.5777 - val_acc: 0.7136\n",
      "Epoch 10/20\n",
      "37500/37500 [==============================] - 218s 6ms/step - loss: 0.4835 - acc: 0.7366 - val_loss: 0.6370 - val_acc: 0.6895\n",
      "Epoch 11/20\n",
      "37500/37500 [==============================] - 222s 6ms/step - loss: 0.4571 - acc: 0.7434 - val_loss: 0.6065 - val_acc: 0.6997\n",
      "Epoch 12/20\n",
      "37500/37500 [==============================] - 215s 6ms/step - loss: 0.4373 - acc: 0.7518 - val_loss: 0.6457 - val_acc: 0.6976\n",
      "Epoch 13/20\n",
      "37500/37500 [==============================] - 218s 6ms/step - loss: 0.4240 - acc: 0.7557 - val_loss: 0.6333 - val_acc: 0.7071\n",
      "Epoch 14/20\n",
      "37500/37500 [==============================] - 216s 6ms/step - loss: 0.4085 - acc: 0.7634 - val_loss: 0.5760 - val_acc: 0.7216\n",
      "Epoch 15/20\n",
      "37500/37500 [==============================] - 218s 6ms/step - loss: 0.3856 - acc: 0.7700 - val_loss: 0.5665 - val_acc: 0.7266\n",
      "Epoch 16/20\n",
      "37500/37500 [==============================] - 220s 6ms/step - loss: 0.3844 - acc: 0.7704 - val_loss: 0.5459 - val_acc: 0.7246\n",
      "Epoch 17/20\n",
      "37500/37500 [==============================] - 217s 6ms/step - loss: 0.3710 - acc: 0.7766 - val_loss: 0.5819 - val_acc: 0.7206\n",
      "Epoch 18/20\n",
      "37500/37500 [==============================] - 216s 6ms/step - loss: 0.3643 - acc: 0.7795 - val_loss: 0.6038 - val_acc: 0.7157\n",
      "Epoch 19/20\n",
      "37500/37500 [==============================] - 220s 6ms/step - loss: 0.3553 - acc: 0.7824 - val_loss: 0.5606 - val_acc: 0.7226\n",
      "Epoch 20/20\n",
      "37500/37500 [==============================] - 220s 6ms/step - loss: 0.3471 - acc: 0.7854 - val_loss: 0.6007 - val_acc: 0.7223\n",
      "10000/10000 [==============================] - 17s 2ms/step\n",
      "Test loss: 0.5857684646010399\n",
      "Test accuracy: 0.7274\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# data_loading code from the official website\n",
    "def load_cifar10_batch(cifar10_dataset_fold_path, batch_id):\n",
    "    file = open(cifar10_dataset_fold_path + '/data_batch_' + str(batch_id), 'rb')\n",
    "    batch = pickle.load(file, encoding='latin1')\n",
    "    features = batch['data'].reshape(len(batch['data']), 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "    file.close()\n",
    "    return features, labels\n",
    "\n",
    "# load training data\n",
    "cifar10_dataset_fold_path = '/Users/dayao/Desktop/data_mining/final_project/cifar-10-batches-py'\n",
    "X_train, y_train = load_cifar10_batch(cifar10_dataset_fold_path, 1)\n",
    "for i in range(2, 6):\n",
    "    features, labels = load_cifar10_batch(cifar10_dataset_fold_path, i)\n",
    "    X_train = np.concatenate((X_train, features))\n",
    "    y_train = np.concatenate((y_train, labels))\n",
    "    \n",
    "# load test data\n",
    "file = open(cifar10_dataset_fold_path + '/test_batch', 'rb')\n",
    "batch = pickle.load(file, encoding='latin1')\n",
    "X_test = batch['data'].reshape(len(batch['data']), 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "y_test = batch['labels']\n",
    "file.close()\n",
    "\n",
    "# normalize the data\n",
    "minmax = MinMaxScaler()\n",
    "X_train_temp = X_train.reshape(X_train.shape[0], 32 * 32 * 3)\n",
    "X_test_temp = X_test.reshape(X_test.shape[0], 32 * 32 * 3)\n",
    "X_train = minmax.fit_transform(X_train_temp)\n",
    "X_test = minmax.fit_transform(X_test_temp)\n",
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 3)\n",
    "\n",
    "# one-hot encode the labels\n",
    "num_classes = 10\n",
    "nums_class = []\n",
    "for j in range(1, num_classes + 1):\n",
    "    nums_class.append(j)\n",
    "lb = LabelBinarizer().fit(nums_class)\n",
    "y_train = lb.transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "# split training data and validation data\n",
    "train_size = 0.75\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  train_size = train_size,\n",
    "                                                  random_state = 7)\n",
    "\n",
    "# build a simple CNN\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "data_augmentation = False\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# fit the data to the model\n",
    "if not data_augmentation:\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(X_val, y_val),\n",
    "              shuffle=True)\n",
    "else:\n",
    "    datagen = ImageDataGenerator(rotation_range=90,\n",
    "                                 width_shift_range=0.1,\n",
    "                                 height_shift_range=0.1,\n",
    "                                 horizontal_flip=True,\n",
    "                                 vertical_flip=True)\n",
    "    datagen.fit(X_train)\n",
    "    model.fit_generator(datagen.flow(X_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        workers=4,\n",
    "                        shuffle=True)\n",
    "\n",
    "# score the trained model\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\my_own_app\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_13 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                20490     \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 307,498\n",
      "Trainable params: 307,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "\n",
    "# data_loading code from the official website\n",
    "def load_cifar10_batch(cifar10_dataset_fold_path, batch_id):\n",
    "    file = open(cifar10_dataset_fold_path + '/data_batch_' + str(batch_id), 'rb')\n",
    "    batch = pickle.load(file, encoding='latin1')\n",
    "    features = batch['data'].reshape(len(batch['data']), 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "    file.close()\n",
    "    return features, labels\n",
    "\n",
    "# load training data\n",
    "cifar10_dataset_fold_path = '/Users/dayao/Desktop/data_mining/final_project/cifar-10-batches-py'\n",
    "X_train, y_train = load_cifar10_batch(cifar10_dataset_fold_path, 1)\n",
    "for i in range(2, 6):\n",
    "    features, labels = load_cifar10_batch(cifar10_dataset_fold_path, i)\n",
    "    X_train = np.concatenate((X_train, features))\n",
    "    y_train = np.concatenate((y_train, labels))\n",
    "    \n",
    "# load test data\n",
    "file = open(cifar10_dataset_fold_path + '/test_batch', 'rb')\n",
    "batch = pickle.load(file, encoding='latin1')\n",
    "X_test = batch['data'].reshape(len(batch['data']), 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "y_test = batch['labels']\n",
    "file.close()\n",
    "\n",
    "# normalize the data\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "minmax = MinMaxScaler()\n",
    "X_train_temp = X_train.reshape(X_train.shape[0], 32 * 32 * 3)\n",
    "X_test_temp = X_test.reshape(X_test.shape[0], 32 * 32 * 3)\n",
    "X_train_HD = minmax.fit_transform(X_train_temp)\n",
    "X_test_HD = minmax.fit_transform(X_test_temp)\n",
    "\n",
    "# # determine the components number of PCA on training data\n",
    "# pca_train = PCA().fit(X_train_HD)\n",
    "# # plot the cumulative summation of the explained variance\n",
    "# plt.figure()\n",
    "# plt.plot(np.cumsum(pca_train.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Variance (%)') # for each component\n",
    "# plt.title('Cifar-10 Explained Variance of training data')\n",
    "# plt.show() # we cawn see when the number of components is 675, we can preserve something around 99.28% of the total variance of the data\n",
    "\n",
    "# # do the PCA, based on the figures, we choose 600 to be the number of components\n",
    "# pca = PCA(n_components=300)\n",
    "# X_train = pca.fit_transform(X_train_HD)\n",
    "# X_test = pca.transform(X_test_HD)\n",
    "\n",
    "# X_train = X_train.reshape(X_train.shape[0], 10, 10, 3)\n",
    "# X_test = X_test.reshape(X_test.shape[0], 10, 10, 3)\n",
    "\n",
    "# one-hot encode the labels\n",
    "num_classes = 10\n",
    "nums_class = []\n",
    "for j in range(1, num_classes + 1):\n",
    "    nums_class.append(j)\n",
    "lb = LabelBinarizer().fit(nums_class)\n",
    "y_train = lb.transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "# split training data and validation data\n",
    "train_size = 0.75\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,\n",
    "                                                  train_size = train_size,\n",
    "                                                  random_state = 7)\n",
    "\n",
    "# build a simple CNN\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "data_augmentation = True\n",
    "weight_decay = 1e-4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay),\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                 kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= 'rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# # fit the data to the model\n",
    "# if not data_augmentation:\n",
    "#     model.fit(X_train, y_train,\n",
    "#               batch_size=batch_size,\n",
    "#               epochs=epochs,\n",
    "#               validation_data=(X_val, y_val),\n",
    "#               shuffle=True)\n",
    "# else:\n",
    "#     datagen = ImageDataGenerator(rotation_range=45,\n",
    "#                                  width_shift_range=0.2,\n",
    "#                                  height_shift_range=0.2,\n",
    "#                                  zoom_range=0.2,\n",
    "#                                  vertical_flip=True,\n",
    "#                                  fill_mode='nearest')\n",
    "#     datagen.fit(X_train)\n",
    "#     model.fit_generator(datagen.flow(X_train, y_train,\n",
    "#                                      batch_size=batch_size),\n",
    "#                         steps_per_epoch=len(datagen.flow(X_train, y_train,batch_size=batch_size)),\n",
    "#                         epochs=epochs,\n",
    "#                         validation_data=(X_val, y_val),\n",
    "#                         workers=4,\n",
    "#                         shuffle=True)\n",
    "\n",
    "# # score the trained model\n",
    "# scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "# print('Test loss:', scores[0])\n",
    "# print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
